<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ops on @jeqo</title>
    <link>https://jeqo.github.io/tags/ops/</link>
    <description>Recent content in ops on @jeqo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; Copyright @jeqo</copyright>
    <lastBuildDate>Sat, 24 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jeqo.github.io/tags/ops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ingesting log files to sqlite</title>
      <link>https://jeqo.github.io/notes/2022-09-24-ingest-logs-sqlite/</link>
      <pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2022-09-24-ingest-logs-sqlite/</guid>
      <description>I recently was looking into how to analyze multiple related log files (e.g. application log from multiple instances), and found that sqlite may be enough :)
The first step is ingesting content from log files into sqlite tables. sqlite-utils to the rescue! I was initially happy with having each line as a row and adding full-text support to the log column to query events. However, a Java log may span across multiple lines and the outputs may not be ideal — timestamps could be in 1 line, and the stack trace root cause in another one.</description>
    </item>
    
    <item>
      <title>Getting started with Kafka quotas</title>
      <link>https://jeqo.github.io/posts/2022-05-11-kafka-quotas/</link>
      <pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/posts/2022-05-11-kafka-quotas/</guid>
      <description>&lt;p&gt;Kafka quotas have been around for a while since initial versions of the project — though not necessarily being enabled in most deployments that I have seen.&lt;/p&gt;
&lt;p&gt;This post shares some thoughts on how to start adopting quotas and gives some practical advice, and a bit of the history of quotas in the Kafka project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Enable Certificate Revocation on Kafka clusters</title>
      <link>https://jeqo.github.io/notes/2022-02-09-kafka-ssl-crl/</link>
      <pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2022-02-09-kafka-ssl-crl/</guid>
      <description>&lt;p&gt;Recently I got a question on how to manage revoked SSL certificates in Kafka clusters.
With a proper Public Key Infrastructure, a Certificate Revocation List (CRL) can be available for clients to validate if a certificate is still valid regardless of its time-to-live.
For instance, if a private key has been compromised, then a certificate can be revoked before it&amp;rsquo;s valid date.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>reload4j as drop-in replacement for log4j 1.x</title>
      <link>https://jeqo.github.io/notes/2022-01-25-reload4j/</link>
      <pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2022-01-25-reload4j/</guid>
      <description>&lt;p&gt;TIL there is a drop-in replacement for log4j 1.x: &lt;a href=&#34;https://reload4j.qos.ch/&#34;&gt;Reload4j&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ansible has a debug mode to pause and troubleshoot</title>
      <link>https://jeqo.github.io/notes/2022-01-21-ansible-debug/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2022-01-21-ansible-debug/</guid>
      <description>&lt;p&gt;I have been running Ansible for a while now.
My usual/naive way of debugging has always been adding a &lt;code&gt;debug&lt;/code&gt; module&lt;a href=&#34;https://docs.ansible.com/ansible/latest/collections/ansible/builtin/debug_module.html&#34;&gt;[1]&lt;/a&gt;, and get the execution running til that point.&lt;/p&gt;
&lt;p&gt;I figured that there are better ways to deal with this&lt;a href=&#34;https://docs.ansible.com/ansible/2.9/user_guide/playbooks_debugger.html#examples&#34;&gt;[2]&lt;/a&gt;.
By using the debug mode, tasks will stop when failing (by default) and you&amp;rsquo;ll be able to introspect into the task, variables, and context when things failed.
Even better, you&amp;rsquo;ll be able to re-execute if there was a transient error.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Changing Kafka Broker&#39;s rack</title>
      <link>https://jeqo.github.io/notes/2021-12-10-kafka-change-rack/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2021-12-10-kafka-change-rack/</guid>
      <description>&lt;p&gt;Kafka broker configuration includes a &lt;code&gt;rack&lt;/code&gt; label to define the location of the broker.
This is useful when placing replicas across the cluster to ensure replicas are spread across locations &lt;em&gt;as evenly as possible&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kafka data loss scenarios</title>
      <link>https://jeqo.github.io/posts/2021-12-10-kafka-data-loss/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/posts/2021-12-10-kafka-data-loss/</guid>
      <description>Kafka topic partitions are replicated across brokers. Data loss happens when the brokers where replicas are located are unavailable or have fully failed. The worst scenario — and where is no much to do — is when all the brokers fail; then no remediation is possible. Replication allows to increase redundancy so this scenarios is less likely to happen.
The following scenarios show different trade-offs that could increase the risk of lossing data:</description>
    </item>
    
    <item>
      <title>Reducing `acks` doesn&#39;t help to reduce end-to-end latency</title>
      <link>https://jeqo.github.io/notes/2021-12-09-kafka-reducing-acks-and-latency/</link>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2021-12-09-kafka-reducing-acks-and-latency/</guid>
      <description>&lt;p&gt;Kafka Producers enforce durability across replicas by setting &lt;code&gt;acks=all&lt;/code&gt; (&lt;a href=&#34;https://jeqo.github.io/til/kafka-v3-idea&#34;&gt;default since v3.0&lt;/a&gt;).
As enforcing this guarantee requires waiting for replicas to sync, this increases latency; and reducing it tends to give the impression that latency gets reduced overall.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Use min.insync.replicas for fault-tolerance</title>
      <link>https://jeqo.github.io/notes/2021-12-02-kafka-min-isr/</link>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jeqo.github.io/notes/2021-12-02-kafka-min-isr/</guid>
      <description>&lt;p&gt;Things to remember:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topic replication factor is not enough to guarantee fault-tolerance. If &lt;code&gt;min.insync.replicas&lt;/code&gt; is not defined i.e. 1, then data could potentially be lost.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acks=all&lt;/code&gt; will force replica leader to wait for &lt;em&gt;all&lt;/em&gt; brokers in the ISR, not only the &lt;code&gt;min.insync.replicas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If replicas available are equal to minimum ISR, then the topic partitions are at the edge of losing availability. If one broker becomes unavailable (e.g. restarting), then producers will fail to write data.&lt;/li&gt;
&lt;li&gt;Topic configuration is inherited from the server. If broker configuration changes, it affects the &lt;em&gt;existing&lt;/em&gt; topics. Keep the topic defaults, unless it needs to be different than broker default for easier maintenance.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
